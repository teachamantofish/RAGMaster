{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p>Code faster, better, and cheaper: </p> <ul> <li>Intelligent web scraping: Tuned web scraping of any document type. </li> <li>Chunking tuned for technical documents.</li> <li>Vectorize document chunks stored in open source dB and tuned for specific domains. </li> <li>dB of rich sources for tuning context and prompts so my coding agent can behave as a domain expert.</li> <li>Agentic rag (example: https://www.youtube.com/watch?v=c5jHhMXmXyo&amp;t=173s and blog: https://weaviate.io/blog/what-is-agentic-rag</li> </ul> <p>Note At a high level these documents are up to date. However, I'm not trying to keep all the details current because the scripts change daily as a result of testing and encountering new issues with newly added docs.</p>"},{"location":"index.html#architectural-decisions","title":"Architectural decisions","text":"<ul> <li>Convert only markdown. Convert all file types to markdown. </li> <li>Target a 400 token chunk size, but don't be too picky. Source docs are random in quality and nature. Usually split code chunks; keep code together unless query responses are imprecise.</li> <li>Design a schema that supports future use of re-ranking and graphrag.</li> <li>Don't use any code or rely on any Youtube video more than 4 months old!</li> <li>Use only free and open source tools: Python, Crawl4AI, LlamaIndex, PostgreSQL</li> <li> <ul> <li>Place parameters in separate config files so settings and script execution can be controlled via a web UI.</li> </ul> </li> <li>Pipeline phases will be in discrete scripts: <ul> <li>crawl_config.py: pipeline configs</li> <li>crawler.py: get and preprocess data. Convert all formats to markdown</li> <li>chunker.py: Chunk, summarize, and create embeddings</li> <li>vector.py: Push to database</li> <li>query.py: Get answers from the database</li> </ul> </li> </ul>"},{"location":"index.html#rag-pipeline-tech-stack","title":"RAG pipeline tech stack","text":"<p>Current pipeline choice: CrawlforAI &gt; Preprocessing (llamaindex + custom) &gt; Chunking (llamaindex) &gt; postprocessing (llamaindex) &gt; openAI embedding (OpenAI via (llamaindex)&gt; store in postgreSQL/pgvector dB &gt; custom web UI</p> <p>Future</p> <ul> <li>DONE: Support PDF</li> <li>DONE: Support Word</li> <li>Investigate Docling, Pandoc, unstructured.io</li> <li>Improve dB with pgscale or timescale</li> <li>Add llamaindex reranking</li> <li>Add agentic rag</li> <li>web app UI.</li> </ul>"},{"location":"chunk_tasks.html","title":"Archive planning tasks file","text":"<ol> <li>Create a <code>chunk_markdown.py</code> file.</li> <li>Import the necessary libraries:</li> <li>Import <code>Path</code> from <code>pathlib</code> for handling file and directory paths in a cross-platform way.</li> <li>Import <code>SimpleDirectoryReader</code> from <code>llama_index.readers.file</code> to read markdown files from a directory.</li> <li>Import <code>HierarchicalNodeParser</code> from <code>llama_index.node_parser</code> to maintain parent-child relationships and support future graphRAG.</li> <li>Import <code>CodeSplitter</code> from <code>llama_index.node_parser</code> to further chunk long code blocks if necessary.</li> <li>Import <code>VectorStoreIndex</code> from <code>llama_index</code> to create vectors for each chunk as they are processed.</li> <li>Import <code>yaml</code> (PyYAML) to read custom metadata from a YAML file.</li> <li>Import <code>openai</code> to generate one-line summaries for each chunk.</li> <li>Import <code>OpenAIEmbedding</code> from <code>llama_index.embeddings</code> to generate vector embeddings for each chunk.</li> <li>Specify the directory containing your markdown files using a <code>Path</code> object.</li> <li>Read the custom metadata from a <code>metadata.yaml</code> file.</li> <li>Use <code>SimpleDirectoryReader</code> to load all markdown files from the directory.</li> <li>Initialize a <code>HierarchicalNodeParser</code> with <code>include_metadata=True</code> (and <code>include_prev_next_rel=True</code> if supported) for chunking the markdown content by heading.</li> <li>For each loaded document, use the parser to chunk the content into nodes, preserving parent-child relationships. Chunking guidelines: </li> <li>Chunk by heading only. We are not using any other chunking method (for example, token length)</li> <li>Headings 1, 2, 3, and 4 should be discrete chunks. </li> <li>Group any headings 5 and 6 with heading 4. </li> <li>Extact any code example over 5 lines and store it as a separate chunk. Reference the code example from the source chunk. </li> <li>Additional metadata should be created for each chunk according to the table below. At a high level: <ul> <li>Get data from metadata.yaml</li> <li>Use openai to create page (heading 1) and chunk summaries</li> <li>Create the rest of the metadata during processing</li> </ul> </li> </ol> Schema Item Source Stored As Definitions document title user supplied metadata metadata.yaml Source may not have it and/or doc cleanup may remove it source url user supplied metadata metadata.yaml Source may not have it and/or doc cleanup may remove it date user supplied metadata metadata.yaml Source may not have it and/or doc cleanup may remove it domain user supplied metadata metadata.yaml Required for separating content in db using pseudo db instances filename file system metadata Read from system id generated metadata Unique UUID for each chunk parent_id generated metadata ID of the parent chunk (if any) header_level generated metadata Inferred from heading. May use this with ranking and structuring id_prev generated metadata Link to prev id_next generated metadata Link to next url_slug generated metadata Optional: markdown-style anchor slug (e.g., #installing-on-windows) examples generated metadata List of code example references ranking generated metadata Pre ranking based on # of matches in parents, children, summary, etc. token count generated metadata Only used if we're interested in checking relative chunk sizes embedding generated vector Optional: store the embedding vector separately heading document chunk Closest header (h1-h4) concat header path generated chunk Header hierarchy path (e.g., \"Getting Started &gt; Installation\") content/text document chunk The chunked text summary generated chunk openai-generated summary of this chunk alone context_summary generated chunk <ol> <li>Initialize the embedding model (e.g., <code>embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")</code>).</li> <li>As you process each chunk, use <code>VectorStoreIndex</code> and the embedding model to create and store the vector for that chunk.</li> <li>Collect all chunks (nodes) into a list for further processing or inspection.</li> <li>Save output chunks and metadata to a specified directory.</li> <li>Print the token size of each chunk to a file in the output directory for review.</li> <li>Test the script to ensure it correctly chunks your markdown files, stores the required metadata, and creates vectors for each chunk.</li> </ol>"},{"location":"chunkingcontent.html","title":"Chunking content","text":"<p>Done.</p> <p>I've decided to stick with llamaindex as much as possible to keep the tech stack simple. There are a number of chunking strategies to choose from, but I prefer a strategy that with a high accuracy relative to some of the more advanced strategies, such as LLM-based chunking. It should also make logical sense given the types of documents I'll be processing.  This research paper is worth a read: https://research.trychroma.com/evaluating-chunking. </p> <p>Since all source documents will be specifications, technical docs, or API references related to developing code, I've decided to use recursive chunking with an approximate chunk size of 400 and no overlap across chunks. The ChromadB research paper suggests a chunk size of about 400 tokens. This is a perfect size since that equates to approximately 15 to 20 lines per chunk at 80 characters wide. That matches fairly well with the likely structure I'll encounter in the technical content available in my source markdown documents. In this case. I expect most content under each heading to fall within the 200-400 token range. My plan is to increase the accuracy by saving to metadata the hierarchical heading path as well as the child headings under the same section. I will also be adding custom metadata during processing.</p> <p></p>"},{"location":"chunkingcontent.html#chunking-strategy","title":"Chunking Strategy","text":"<ul> <li>Metadata</li> <li>Define metadata that cannot be programmatically created in a config file</li> <li>Store metadata in well-defined fields in the same table row as the chunk</li> <li>Use <code>llamaindex</code> <code>MarkdownNodeParser</code>:<ul> <li><code>include_metadata</code> (bool): whether to include metadata in nodes</li> <li><code>include_prev_next_rel</code></li> </ul> </li> <li>Chunking</li> <li>Chunk by heading (assuming structured, technical docs)</li> <li>Don't chunk H5 (keep with H4)</li> <li>If a heading and its child headings are &lt; 500 chars, chunk together</li> <li>Reference code examples from each chunk</li> <li>Reference metadata from each chunk</li> <li>Summaries in metadata:<ul> <li>Summarize each chunk and store in metadata</li> <li>Summarize H1 and store in metadata</li> </ul> </li> </ul>"},{"location":"chunkingcontent.html#code-example-chunking","title":"Code example chunking","text":"<p>This strategy is evolving. . . </p> <ul> <li>Code Handling</li> <li>Use <code>CodeSplitter</code> from <code>llama_index.node_parser</code>:<ul> <li>Store code examples longer than 3 lines separately; reference from text chunk</li> <li>Chunk by function/class</li> </ul> </li> </ul> <pre><code>splitter = CodeSplitter(\n    language='python',          # or 'javascript', 'java', etc.\n    chunk_lines=50,             # Number of lines per chunk\n    chunk_overlap=5             # Overlap lines for continuity\n</code></pre>"},{"location":"chunkingcontent.html#embedding-create-vectors","title":"Embedding (create vectors)","text":"<ul> <li>At embedding time, use an embedding model tuned for code:</li> <li>OpenAI: <code>text-embedding-3-small</code> is OK</li> <li>Use <code>embed_model = OpenAIEmbedding()</code> (creates vector representation)</li> </ul>"},{"location":"chunkingcontent.html#packages-used","title":"Packages used","text":"<pre><code>import os\nimport re\nimport openai\nimport uuid\nimport json\nfrom llama_index.core import SimpleDirectoryReader\nfrom llama_index.core.node_parser import MarkdownNodeParser\nfrom llama_index.core.node_parser.text.code import CodeSplitter\nfrom llama_index.core.settings import Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom pygments.lexers import guess_lexer, ClassNotFound\nfrom collections import OrderedDict\nimport logging\n</code></pre>"},{"location":"code_splitting_thoughts.html","title":"Splitting long code into chunks","text":"<p>I asked several agents what my approach should be with respect to long code examples. I've decided to generate a code example chunk summary via an LLM agent and double the size of that summary in hopes of enhancing retrieval. </p> <p>I've attached my agent responses below.</p>"},{"location":"code_splitting_thoughts.html#question","title":"Question","text":"<p>Some of the code examples Im going to use in an agentic RAG pipeline range from 800 to 1500 tokens long. I'm wondering if I should split them up or leave in one piece. What is going to be the difference for a agentic RAG pipeline with respect to accuracy and precision if I keep a discrete code example unchunked even if it's long?</p> <p>On the one hand, keeping a code example entirely on one chunk seems like it makes sense because the code belongs together and can't function without the other pieces. On the other hand, a lot of what I'm reading says the chunk should be around 400 tokens. What are your thoughts? ChatGPT said:</p>"},{"location":"code_splitting_thoughts.html#agent-answer","title":"Agent answer","text":"<p>This is a nuanced tradeoff between semantic coherence and retrieval precision. Since you're building an agentic RAG pipeline (where an LLM may act in a planning/execution loop), the stakes for chunking strategy are a bit higher. Here's how to think about it: \ud83e\udde9 Keeping Long Code Examples Intact (800\u20131500 tokens)</p> <p>Pros:</p> <pre><code>Semantic coherence: The entire code context is preserved. This is especially important for:\n\n    Functions/classes that span many lines.\n\n    Examples that include setup, execution, and output.\n\nLess hallucination: The LLM sees all relevant parts of the code together, reducing risk of incorrect assumptions.\n\nBetter for reasoning or tool use: If your agent will manipulate or execute code, it needs full access to dependencies and scope.\n</code></pre> <p>Cons:</p> <pre><code>Reduced retriever precision: Vector similarity search becomes less granular. A query might match a large chunk even if only a small part is relevant.\n\nEmbedding dilution: Embeddings are an averaged representation; long chunks can blur focus.\n\nMay not fit in context: Especially if you're assembling multiple chunks in a prompt for the LLM.\n</code></pre> <p>\u2702\ufe0f Splitting Long Code into Smaller Chunks (~400 tokens)</p> <p>Pros:</p> <pre><code>Higher retrieval precision: Smaller chunks are easier to rank and score accurately.\n\nEasier reranking: If you\u2019re using hybrid search (e.g. dense + keyword or reranker), small chunks give finer granularity.\n\nFaster embedding updates: Changing a small part of the code doesn\u2019t require re-embedding a giant chunk.\n</code></pre> <p>Cons:</p> <pre><code>Loss of semantic integrity: Many programming constructs require prior definitions or context.\n\nAgent may need to stitch context: In agentic workflows, this adds planning and increases risk of failure.\n</code></pre> <p>\ud83d\udca1 Hybrid Strategy (Recommended)</p> <p>Use adaptive chunking with the following logic:</p> <pre><code>Keep code examples together if they are tightly coupled and meant to function as a whole. It\u2019s better to keep one 1200-token example intact than split a class or function in two.\n\nUse metadata to help the retriever understand boundaries \u2014 e.g., include \"type\": \"code-example\" or \"function\": \"init_model()\" in metadata.\n\nAugment with summaries: For large chunks, precompute a 100\u2013200 token natural language summary to index alongside. This helps retrieval and reranking.\n\nEnable cross-chunk linking or stitching: If you do split, build logic into your RAG pipeline to allow pulling in neighboring chunks if needed (e.g. via parent-child or sibling references).\n</code></pre> <p>\ud83d\udee0 Agentic RAG-Specific Considerations</p> <p>Since your agent may:</p> <pre><code>Select tools based on code examples.\n\nModify, run, or generate similar code.\n</code></pre> <p>You should bias toward keeping semantically complete code blocks intact. This will:</p> <pre><code>Make reasoning steps easier.\n\nAllow planning without additional retrieval steps.\n\nAvoid requiring the agent to guess or reconstruct partial logic.\n</code></pre>"},{"location":"datastore.html","title":"Setting up a vector data store","text":"<p>Done.</p> <p>After much research I decided that I do not need a vector database such as Pinecone or Supabase. This decision is based on the fact that I'll probably never index more than 50,000 pages. My chunk token count should stay under 4 million tokens. Therefore, I'm going with open source postgresql and pgvector. </p>"},{"location":"datastore.html#strategy","title":"Strategy","text":"<p>It's worth noting that we only need a single database because disparate content will be identified by metadata. For example, a FrameMaker query doesn't need to know anything about Markdown or PDF tools. Both custom and generated metadata should adequately isolate content by domain. In addition to script-extracted or generated metadata, I will be adding custom metadata that includes the domain, content type, and a short description. For example: \" FrameScript Specification (for manipulating FrameMaker documents).\" The strategy supports:</p> <ul> <li>Semantic Isolation: Vector similarity search should have a high level of retrieval quality since similar embeddings are semantically meaningful in the same context.</li> <li>Query Precision: Reduced noise to increase performance and relevance for agents working in a specific domain.</li> <li>Efficient Resource Use: Small, domain-specific collections are faster to query and easier to manage. Isolated contexts provide a way to modify the architecture as the pipeline evolves.</li> <li>Agent-Specific Retrieval: Specific domains provide a way to use domain-specialized agents and create multi-agent workflows. This keeps the mental model and toolchain cleaner.</li> </ul> <p>LlamaIndex makes this easy:</p> <ul> <li>Use multiple VectorStoreIndex instances, each pointed to its own domain data and vector store.</li> <li>Query the correct index depending on the agent context.</li> <li>Use a single PostgreSQL database.</li> <li>Create a single table for vectors, but include a domain column ('framemaker', 'markdown', etc.).</li> <li>Configure LlamaIndex to use pgvector with filtering by domain at query time.</li> </ul>"},{"location":"datastore.html#schema","title":"Schema","text":"<p>Everything that provides semantic context to the embedding model is stored with the chunk. Other items are stored in the metadata and stored separately. It's important to preserve the hierarchical and logical placement of the chunk relative to the document.</p> Schema Item Source Text type Data Type Definitions custom title user supplied metadata TEXT metadata.yaml Source may not have it and/or doc cleanup may remove it source url user supplied metadata TEXT metadata.yaml Source may not have it and/or doc cleanup may remove it date user supplied metadata TEXT metadata.yaml Source may not have it and/or doc cleanup may remove it domain user supplied metadata TEXT metadata.yaml Required for separating content in dB using pseudo dB instances filename file system metadata TEXT Read from system id generated metadata TEXT Unique UUID for each chunk parent_id generated metadata TEXT ID of the parent chunk (if any) header_level generated metadata INTEGER Inferred from heading. May use this with ranking and structuring id_prev generated metadata TEXT Link to prev id_next generated metadata TEXT Link to next examples generated metadata TEXT List of code example references ranking generated metadata REAL Pre ranking based on # of matches in parents, children, summary, etc. token count generated metadata INTEGER Only used if we're interested in checking relative chunk sizes type generated metadata TEXT Code example only: null if text, \"code_example\" if code language generated metadata TEXT Code example only: The auto-detected language of the code example embedding generated vector vector Optional: store the embedding (1536) vector separately heading document chunk TEXT Closest header (h1-h4) concat header path generated chunk TEXT Header hierarchy path (e.g., \"Getting Started &gt; Installation\") content/text document chunk TEXT The chunked text summary generated chunk TEXT Auto-generated summary of this chunk alone context_summary generated chunk TEXT Generated summary of the parent section for added semantic context"},{"location":"datastore.html#example-data","title":"Example data","text":"<p>Note: The actual schema is better and continues to evolve.</p> <pre><code>{\n  \"id\": \"chunk-0458\",\n  \"embedding\": [0.123, -0.456, 0.789, ...],  // 1536 floats for `text-embedding-3-small`\n  \"text\": \"To install the CLI, run `npm install -g example-cli`...\",\n  \"metadata\": {\n    \"section_header\": \"Installation\",\n    \"full_path\": \"Docs &gt; Getting Started &gt; Installation\",\n    \"summary\": \"Explains how to install the CLI via npm.\",\n    \"context_summary\": \"Covers the setup and configuration for first-time users.\",\n    \"parent_node\": \"chunk-0457\",\n    \"child_nodes\": [\"chunk-0459\", \"chunk-0460\"],\n    \"doc_id\": \"doc-101\",\n    \"title\": \"Getting Started Guide\",\n    \"custom_title\": \"Beginner Setup\",\n    \"source_url\": \"https://example.com/docs/getting-started\",\n    \"file_name\": \"getting-started.md\",\n    \"date\": \"2024-06-01\",\n    \"code_example_ref\": \"example-CLI-install-1\",  // could link to a repo or blob store\n    \"tokens\": 138,\n    \"chunk_index\": 3\n  }\n}\n</code></pre>"},{"location":"datastore.html#vector-database-setup","title":"Vector database setup","text":""},{"location":"datastore.html#install-llamaiindex-packages","title":"Install llamaiindex packages","text":"<ol> <li>pip install llama-index-vector-stores-postgres</li> <li>pip install psycopg2-binary: a Python package that provides a PostgreSQL database adapter for Python. It allows your Python code to connect to and interact with a PostgreSQL database. It is commonly used in Python projects that need to run SQL queries, manage tables, or interact with PostgreSQL in any way.</li> <li>pip install openai: The openai Python package is the official client library for accessing OpenAI's APIs (such as GPT-3, GPT-4, embeddings, etc.) from Python code. It allows you to send requests to OpenAI models (for text generation, embeddings, etc.) and receive responses in your Python programs.</li> </ol>"},{"location":"datastore.html#set-up-postgresql-and-pgvector","title":"Set up postgreSQL and pgvector","text":"<ol> <li>Install https://www.enterprisedB.com/downloads/postgres-postgresql-downloads</li> <li>pip install pgvector (to verify the package) installed (does not install pgvector extension)</li> <li>Compile/build pgvector: \u231bThis step was filled with landmines and pitfalls. There are many install paths, including Docker, VMs, Windows build tools, etc. I didn't want to install 7 GB of Visual Studio files, and my AI agent told me I could use MSYS. However, that rabbit hole cost me several hours. In the end, I installed Visual Studio and followed the instructions in this article, and was up and running in 30 minutes.</li> <li>pwd: xxxx; port 5432</li> <li>Get pgadmin: https://www.postgresql.org/ftp/pgadmin/pgadmin4/v9.4/windows/</li> <li>Verify postgresql server is running (services.msc)</li> <li>Open pgadmin</li> <li>Create a new server</li> <li>Connect to the server</li> <li>Open the query workspace tool</li> <li>Choose the server to query.</li> <li>????</li> </ol>"},{"location":"datastore.html#packages-used","title":"Packages used","text":"<pre><code>import json\nimport logging\nimport sys\nfrom pathlib import Path\nimport psycopg2\n</code></pre> <p>from psycopg2.extras import execute_batch import os</p>"},{"location":"datastore.html#push-to-database","title":"Push to database","text":"<ol> <li>Launch pgAdmin (auto starts the server)</li> <li>Create a server.</li> <li>Right click on the server.</li> <li>Choose Register &gt; Server.</li> <li>Provide a name, host name, port, username</li> <li>Adobe gotcha: Enable the port via Windows Defender Firewall if needed (image below).</li> <li>Select the Server.</li> <li>Select the Query tool.</li> <li>Create the dB.</li> <li>Connecting to the dB took several hours of troubleshooting. Hopefully you're not a novice.</li> <li>Run vectory.py</li> <li>Verify the chunks exist. To view your data in pgAdmin: In the Browser panel, expand Databases &gt; postgres &gt; Schemas &gt; public &gt; Tables. Select Select View/Edit Data &gt; All Rows.</li> </ol> <p>Windows firewall port configuration </p>"},{"location":"gettingcontent.html","title":"Getting content","text":"<p>Done.</p> <p>Scraping the web primarily uses llamaindex with some addons such as BeautifulSoup. I'm choosing to get HTML here even if there's markdown in a repo for the sake of consistency. All sources docs whether HTML, PDF, DOCX, or MD are converted to markdown.</p> <ul> <li>Convert all sources to markdown</li> <li>Clean and normalize markdown (llamaindex, markdowncleaner, and custom regex): </li> <li>Don't remove frontmatter that might be needed for metadata</li> <li>Strip non-semantic syntax; e.g. bold, images, etc.</li> <li>Unwrap hard wrapped lines</li> <li>Remove image links and images, retain alt text</li> <li>HTML cleaning</li> <li>Whitespace normalization</li> <li>Code example processing: retain whitespace. I've chosen to add the previous paragraph line as a comment prefacing the code since that usually is a code example title or summary. </li> </ul>"},{"location":"gettingcontent.html#setup-notes","title":"Setup notes","text":"<ol> <li>Install crawl4AI: https://docs.crawl4ai.com/core/installation/</li> <li>pip install markdowncleaner. This is perhaps more useful for PDF source, but run it anyway just in case.</li> <li>Configure cmd_crawl.py<ol> <li>Set crawl_config.py options. I set a target directory outside the directory where this script runs so my IDE agent does not index it. </li> <li>Set options in recursive_crawl_urls for CrawlerRunConfig and DefaultMarkdownGenerator (from llamaindex)</li> <li>Set options in clean_markdown for MarkdownCleaner. See https://github.com/josk0/markdowncleaner/tree/main/src/markdowncleaner.</li> <li>Create any needed custom regex in custom_regex.</li> </ol> </li> <li>Run <code>python -m cmd_crawl.py</code></li> </ol> <p>Note crawl4ai has many options, so read the docs to solve scraping issues. Pandoc is another good post-crawl processing choice. There does not appear to be a \"one size fits all\" tool so get the tools you need and customize.</p>"},{"location":"gettingcontent.html#packages-used","title":"Packages used","text":"<pre><code>import os  # For directory and file operations\nimport re\nimport asyncio  # For async crawling\nimport argparse  # For command-line argument parsing\nfrom urllib.parse import urljoin, urlparse, urldefrag\nfrom collections import deque\nfrom markdowncleaner import MarkdownCleaner, CleanerOptions # for additional markdown post-processing\nfrom bs4 import BeautifulSoup\nfrom langdetect import detect, LangDetectException\n# Import Crawl4AI core classes and strategies from the installed package\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher\nfrom crawl4ai import CrawlerMonitor, DisplayMode\nfrom crawl4ai.content_filter_strategy import PruningContentFilter\nfrom crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\nfrom crawl4ai import BrowserConfig\n\nimport logging  # For logging\nimport traceback  # For detailed error information\n</code></pre>"},{"location":"sql_cmds.html","title":"List of useful queries","text":"<p>I don't know SQL or db mgmt., so I'm storing examples here: </p>"},{"location":"sql_cmds.html#find-specific-data-by-string","title":"Find specific data by string","text":"<pre><code>SELECT * FROM chunks WHERE content LIKE '%mkdocs%';\n</code></pre>"},{"location":"sql_cmds.html#create-a-database","title":"Create a database","text":"<pre><code>CREATE DATABASE \"BensDocServer\";\n</code></pre>"},{"location":"sql_cmds.html#grant-access","title":"Grant access","text":"<pre><code>GRANT ALL PRIVILEGES ON TABLE public.chunks TO postgres;\n</code></pre>"},{"location":"sql_cmds.html#create-schemacolumns","title":"Create schema/columns","text":"<pre><code>CREATE TABLE chunks (\n    id TEXT PRIMARY KEY,\n    filename TEXT,\n    heading TEXT,\n    header_level INTEGER,\n    concat_header_path TEXT,\n    content TEXT,\n    parent_id TEXT,\n    examples TEXT,\n    token_count INTEGER,\n    chunk_summary TEXT,\n    page_summary TEXT,\n    id_prev TEXT,\n    id_next TEXT,\n    domain TEXT,\n    custom_title TEXT,\n    source TEXT,\n    date TEXT,\n    embedding vector(1536),\n    ranking REAL DEFAULT NULL,\n    type TEXT,\n    language TEXT\n);\n</code></pre>"},{"location":"sql_cmds.html#list-all-tables-in-public-schema","title":"List All Tables in Public Schema","text":"<pre><code>SELECT table_schema, table_name FROM information_schema.tables WHERE table_schema = 'public';\n</code></pre>"},{"location":"sql_cmds.html#list-all-tables-named-chunks","title":"List All Tables Named \"chunks\"","text":"<pre><code>SELECT table_schema, table_name FROM information_schema.tables WHERE table_name = 'chunks';\n</code></pre>"},{"location":"sql_cmds.html#show-table-structure-columns-and-types","title":"Show Table Structure (Columns and Types)","text":"<pre><code>SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'chunks';\n</code></pre>"},{"location":"sql_cmds.html#view-all-rows-in-chunks-table","title":"View All Rows in Chunks Table","text":"<pre><code>SELECT * FROM chunks;\n</code></pre>"},{"location":"sql_cmds.html#view-first-10-rows-in-chunks-table","title":"View First 10 Rows in Chunks Table","text":"<pre><code>SELECT * FROM chunks LIMIT 10;\n</code></pre>"},{"location":"sql_cmds.html#count-rows-in-chunks-table","title":"Count Rows in Chunks Table","text":"<pre><code>SELECT COUNT(*) FROM chunks;\n</code></pre>"},{"location":"sql_cmds.html#delete-a-row-by-id","title":"Delete a Row by ID","text":"<pre><code>DELETE FROM chunks WHERE id = 'the_accidental_id';\n</code></pre>"},{"location":"sql_cmds.html#filter-data-by-string-in-a-column","title":"Filter Data by String in a Column","text":"<pre><code>SELECT * FROM chunks WHERE content LIKE '%xxx%';\n</code></pre>"},{"location":"unusedideas.html","title":"Unused ideas","text":"<p>Might do later. </p>"},{"location":"unusedideas.html#groupingheadingstoreach400tokens","title":"groupingHeadingsToReach400Tokens","text":"<p>Key points: - This logic always includes the full content of each heading. - It merges as many headings as possible into a chunk, up to the token limit. - It never splits a heading\u2019s content. - You can still extract code examples and generate summaries for each chunk as before, just use combined_text instead of node.text.</p> <p>How it works: - It starts a chunk with the current heading. - It tries to add as many subsequent headings as possible without exceeding 400 tokens. - It never splits a heading\u2019s content. - When the next heading would push the chunk over 400 tokens, it starts a new chunk.</p> <pre><code>MAX_TOKENS = 400\n\nfor doc in documents:\n    nodes = markdown_parser.get_nodes_from_documents([doc])\n    i = 0\n    n = len(nodes)\n    prev_id = None\n\n    while i &lt; n:\n        # Start a new chunk with the current node\n        chunk_nodes = [nodes[i]]\n        chunk_tokens = len(TOKENIZER.encode(nodes[i].text))\n        j = i + 1\n\n        # Try to add as many subsequent nodes as possible without exceeding MAX_TOKENS\n        while j &lt; n:\n            next_tokens = len(TOKENIZER.encode(nodes[j].text))\n            if chunk_tokens + next_tokens &gt; MAX_TOKENS:\n                break  # Adding this node would exceed the limit\n            chunk_nodes.append(nodes[j])\n            chunk_tokens += next_tokens\n            j += 1\n\n        # Combine the content and metadata for the chunk\n        chunk_id = str(uuid.uuid4())\n        combined_text = \"\\n\\n\".join(node.text for node in chunk_nodes)\n        chunk = {\n            \"id\": chunk_id,\n            \"filename\": doc.metadata.get(\"file_path\", \"unknown.md\"),\n            \"heading\": chunk_nodes[0].metadata.get(\"heading\", \"\"),\n            \"header_level\": chunk_nodes[0].metadata.get(\"header_level\", None),\n            \"concat_header_path\": chunk_nodes[0].metadata.get(\"header_path\", \"\"),\n            \"content\": combined_text,\n            \"parent_id\": chunk_nodes[0].metadata.get(\"parent_id\", None),\n            \"examples\": [],\n            \"token_count\": chunk_tokens,\n            \"summary\": \"\",    # to be filled\n            \"context_summary\": \"\",  # to be filled\n            \"id_prev\": prev_id,\n            \"id_next\": None,  # to be filled after loop\n            \"embedding\": None, # to be filled\n        }\n        # Attach user metadata\n        chunk.update(user_metadata)\n\n        # (You can keep your code example extraction and summary logic here, using combined_text)\n\n        all_chunks.append(chunk)\n        if prev_id is not None:\n            chunk_id_map[prev_id][\"id_next\"] = chunk_id\n        chunk_id_map[chunk_id] = chunk\n        prev_id = chunk_id\n\n        i = j  # Move to the next unprocessed node\n</code></pre>"},{"location":"usedata.html","title":"Use the data","text":"<p>There are several ways to retrieve and use the data, each with varying levels of complexity. I will implement them one by one here.</p> <ul> <li>llamaiindex retriever: Query the database and get results based on a semantic match. No AI or LLM is involved. </li> <li>llamaindex local chatbot</li> <li>Using agent outside of Cursor and use my personal OpenAI key</li> <li>Build a custom agent that interacts with a web UI to query my database and return results.</li> <li>Use the process above but automatically feed those results along with other prompt instructions to the agent inside of Cursor using an MCP server. </li> <li>(Don't know if I'll do this). Instead of using my personal OpenAI key for the agent usage above, install an LLM locally and run the agent locally.</li> </ul>"},{"location":"usedata.html#tbd-user-interface-for-pipeline-config","title":"TBD: User interface for pipeline config","text":"<p>I built a test Crawl4AI Multi-URL Crawler Flask App that works, but of course the entire pipeline needs to work first. This is on hold until the other pieces are complete.</p> <p>This app crawls multiple specified URLs and their subpages using Crawl4AI and displays the results in a simple web UI.</p>"},{"location":"usedata.html#setup-future-work","title":"Setup (future work)","text":"<p>Run it all from a web UI. . . </p> <ol> <li>Install dependencies:    <pre><code>pip install -r requirements.txt\n</code></pre></li> <li>Run the Flask app:    <pre><code>python app.py\n</code></pre></li> <li>Open your browser to http://localhost:5000 and click \"Start Crawl\".</li> </ol>"}]}